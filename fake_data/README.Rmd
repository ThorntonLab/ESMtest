#A tool to take [ms](http://home.uchicago.edu/rhudson1/source/mksamples.html) output and create a PED/MAP file pair for use with PLINK.

#Example use:  make 3,000 cases + 3,000 controls and 500 SNPs.  That is 6,000 diploids, so 12,000 chromosomes.  Using ms:

```{sh}
ms 12000 1 -s 500 | ./ms2plink fake.ped fake.map
```

##Then, to get 1 million permutations:

```{sh}
#We use --map3 b/c we do not have the 3rd column in the map file, which is position in cM.
plink --file fake --assoc --map3 --mperm 1000000 --mperm-save-all
```


Full example of a workflow:

```{sh}
#Simulate some data.
ms 12000 1 -s 500 | ./ms2plink fake.ped fake.map
#Make binary input files
../../plink_test/plink-1.07-srcKRT/plink --file fake --make-bed --map3 --out fake
#Do perms
../../plink_test/plink-1.07-srcKRT/plink --bfile fake --assoc --map3 --mperm 200000 --mperm-save-all --hwe 1e-6 --out fake
```

The above are executed in the script called fakeit.sh.


#Requirements on HPC:

```
module load krthornt/libsequence/1.8.0
```

#Results

I ran the script testperms.sh, which permuts 250 snps 1 million times.

```
#!/bin/bash

#$ -q krt
#$ -t 1-64

module load krthornt/plsink/1.07
module load krthornt/libsequence/1.8.0

cd $SGE_O_WORKDIR
SEED1=`echo "$SGE_TASK_ID*$RANDOM"|bc -l`
SEED3=`echo "$SGE_TASK_ID*$RANDOM"|bc -l`
SEED2=`echo "$SGE_TASK_ID*$RANDOM"|bc -l`

ms 12000 1 -s 250 -seed $SEED1 $SEED2 $SEED3 | ~/src/ESMtest/fake_data/ms2plink testdata.$SGE_TASK_ID.ped testdata.$SGE_TASK_ID.map
#Make binary input files
plink --noweb --file testdata.$SGE_TASK_ID --make-bed --map3 --out testdata.$SGE_TASK_ID --silent
#Do perms
/usr/bin/time -f "%e %M" -o ptime1million.$SGE_TASK_ID.txt plink --noweb --bfile testdata.$SGE_TASK_ID --assoc --map3 --mperm 1000000 --mperm-save-all --hwe 1e-6 --out testdata.$SGE_TASK_ID --silent

#We cannot suppress the log file, but we can delete it:
rm -f testdata.$SGE_TASK_ID.log

#After running a job like this, the original .bed and .map files should probably be gzipped and tarred up
```

The distribution of run times looks like:

```{r,echo=FALSE}
x=read.table("times.txt")
hist(x$V1/(24*60^2),xlab="Run time (days)",main="")
meanrtime=mean(x$V1/(24*60^2))
```

The mean run time is `r meanrtime` days, with a max of `r max(x$V1/(24*60^2))` days.

Given that we will need to do 3 million perms, and we have a 3 day run-time limit on the pub64 queue, it is probably safest to limit the permutations to 175 or 200 SNP chunkks (as run time should be linear in the number of markers).

What happens if we use PLINK 1.90a?  The answer is that it is a ton faster:

```{r,echo=FALSE}
x.p190 = read.table("timesP190.txt")
hist(x.p190$V1/(60),xlab="Run time (minutes)",main="")
```

Now, the mean run time is `r mean(x.p190$V1/60)` __minutes__.  

There is a massive difference in RAM use, though:

```{r,echo=FALSE}
par(mfrow=c(1,2))
hist(x$V2/(4*1024),xlab="Peak RAM use (MB)",main="PLINK 1.07")
hist(x.p190$V2/(4*1024),xlab="Peak RAM use (MB)",main="PLINK 1.90a")
```

OK, so the new PLINK makes up the difference by using tons of RAM.   Does it get worse if we do all 3 million perms in 1 run?

This is again for 250 markers in a sample of 3000 controls + 3000 cases, but now 3e6 permutations:

```{r,echo=FALSE}
x.p1903e6=read.table("timesP1903e6.txt")
par(mfrow=c(1,2))
hist(x.p1903e6$V1/(60),xlab="Run time (minutes)",main="PLINK 1.90a, 3e6 perms")
hist(x.p1903e6$V2/(4*1024),xlab="Peak memory (MB)",main="PLINK 1.90a, 3e6 perms")
```

So, that is pretty amazing.  Mean run time only goes up to `r mean(x.p1903e6$V1/(60))` minutes.  RAM use goes through the roof, though.

__IMPORTANT:__ PLINK1.90a's permutation files contain the observed statistics on the first line!!!!!!!  These will need to be skipped!!!

The script generating the last figure is:
```{sh}
#!/bin/bash

#$ -N PERMS
#$ -q krt
#$ -t 1-64
#$ -pe openmp 4
#module load krthornt/plink/1.07
module load plink/1.90a
module load krthornt/libsequence/1.8.0

cd $SGE_O_WORKDIR
SEED1=`echo "$SGE_TASK_ID*$RANDOM"|bc -l`
SEED3=`echo "$SGE_TASK_ID*$RANDOM"|bc -l`
SEED2=`echo "$SGE_TASK_ID*$RANDOM"|bc -l`

ms 12000 1 -s 250 -seed $SEED1 $SEED2 $SEED3 | ~/src/ESMtest/fake_data/ms2plink testdata.$SGE_TASK_ID.ped testdata.$SGE_TASK_ID.map
#Make binary input files
plink --noweb --file testdata.$SGE_TASK_ID --make-bed --map3 --out testdata.$SGE_TASK_ID --silent
#Do perms
/usr/bin/time -f "%e %M" -o ptime1millionP190.$SGE_TASK_ID.txt plink --noweb --bfile testdata.$SGE_TASK_ID --assoc --map3 --mperm 3000000 --mperm-save-all --hwe 1e-6 --out testdata.$SGE_TASK_ID --silent

#We cannot suppress the log file, but we can delete it:
rm -f testdata.$SGE_TASK_ID.log

#After running a job like this, the original .bed and .map files should probably be gzipped and tarred up
```

The bounds on permutation on HPC
===
In the last section, 3e6 perms of 250 markers in 6k individuals is `r 3e6*250` calculations of the chi-squared statistic.  In the WTCCC data, chr2 has the larged number of markers, which is less than 50,000.  If we assum that resources is a function of the total number of calculations done, then we predict we can do `r 3e6*250/5e4` permutations of a data set with 50k markers.  If so, then we would require `r 3e6/1.5e4` array job tasks to permute each chromosome, and we would generate that many __random number seeds__ ahead of time, so that each chromosome's permutation order is the same.

OK, so here are the results of permuting 6,000 individuals with 50,000 makers 15,000 times:

```{r,echo=FALSE}
par(mfrow=c(1,2))
big=read.table("bigperms.txt")
hist(big$V1/60,xlab="Run time (minutes)",main="Big permutation")
hist(big$V2/(4*1024),xlab="Peak RAM use (MB)",main="Big permutation")
```

Mean run time is `r mean(big$V1/60)`, and RAM use is totally fine.  The down side is that the program does much more writing to disk, which bogs things down.  It may be useful to try a much bigger job and see if and /fast-scratch and/or /dfs1 can handle it.  This is clearly the _easiest_ procedure, as there will be less to "stitch" together after doing the perms.